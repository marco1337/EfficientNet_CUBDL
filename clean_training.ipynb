{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from cubdl.PixelGrid import make_pixel_grid\n",
    "from cubdl.das_torch import DAS_PW\n",
    "import h5py\n",
    "from cubdl.PlaneWaveData import PlaneWaveData\n",
    "from glob import glob\n",
    "from scipy.signal import hilbert, convolve\n",
    "from efficientnet_lite import efficientnet_lite_params, build_efficientnet_lite\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_and_acq(filename):\n",
    "    file_text = os.path.splitext(filename)\n",
    "    name = file_text[0]\n",
    "    number = int(name[-3:])\n",
    "    origin = name[0:3]\n",
    "    return origin, number\n",
    "\n",
    "def read_us_images(directory):\n",
    "    files = os.listdir(directory)\n",
    "    return files\n",
    "\n",
    "class TSHData(PlaneWaveData):\n",
    "    \"\"\" Load data from Tsinghua University. \"\"\"\n",
    "\n",
    "    def __init__(self, database_path, acq):\n",
    "        # Make sure the selected dataset is valid\n",
    "        moniker = \"TSH{:03d}\".format(acq) + \"*.hdf5\"\n",
    "        fname = [\n",
    "            y for x in os.walk(database_path) for y in glob(os.path.join(x[0], moniker))\n",
    "        ]\n",
    "        assert fname, \"File not found.\"\n",
    "\n",
    "        # Load dataset\n",
    "        f = h5py.File(fname[0], \"r\")\n",
    "\n",
    "        # Get data\n",
    "        self.angles = np.array(f[\"angles\"])\n",
    "        self.idata = np.array(f[\"channel_data\"], dtype=\"float32\")\n",
    "        self.idata = np.reshape(self.idata, (128, len(self.angles), -1))\n",
    "        self.idata = np.transpose(self.idata, (1, 0, 2))\n",
    "        self.qdata = np.imag(hilbert(self.idata, axis=-1))\n",
    "        self.fc = np.array(f[\"modulation_frequency\"]).item()\n",
    "        self.fs = np.array(f[\"sampling_frequency\"]).item()\n",
    "        self.c = 1540  # np.array(f[\"sound_speed\"]).item()\n",
    "        self.time_zero = np.zeros((len(self.angles),), dtype=\"float32\")\n",
    "        self.fdemod = 0\n",
    "\n",
    "        # Make the element positions based on L11-4v geometry\n",
    "        pitch = 0.3e-3\n",
    "        nelems = self.idata.shape[1]\n",
    "        xpos = np.arange(nelems) * pitch\n",
    "        xpos -= np.mean(xpos)\n",
    "        self.ele_pos = np.stack([xpos, 0 * xpos, 0 * xpos], axis=1)\n",
    "\n",
    "        # For this dataset, time zero is the center point\n",
    "        for i, a in enumerate(self.angles):\n",
    "            self.time_zero[i] = self.ele_pos[-1, 0] * np.abs(np.sin(a)) / self.c\n",
    "\n",
    "        # Validate that all information is properly included\n",
    "        super().validate()\n",
    "\n",
    "def USData_1angle(root_dir,file):\n",
    "    print(\"1angle : \" + str(file))\n",
    "    origin, acq = get_name_and_acq(file)\n",
    "    # print(os.path.abspath(file))\n",
    "    full_path = os.path.abspath(file)\n",
    "    if origin == 'TSH':\n",
    "        P = TSHData(root_dir, acq)\n",
    "        xlims = [P.ele_pos[0, 0], P.ele_pos[-1, 0]]\n",
    "        zlims = [10e-3, 45e-3]\n",
    "\n",
    "    wvln = P.c / P.fc\n",
    "    dx = wvln / 2.5\n",
    "    dz = dx  # Use square pixels\n",
    "    grid = make_pixel_grid(xlims, zlims, dx, dz)\n",
    "    fnum = 1\n",
    "\n",
    "    # make data from 1 angle\n",
    "    x = (P.idata, P.qdata)\n",
    "    idx = len(P.angles) // 2  # Choose center angle\n",
    "    das1 = DAS_PW(P, grid, idx, rxfnum=fnum)\n",
    "    idas1, qdas1 = das1(x)\n",
    "    idas1, qdas1 = idas1.detach().cpu().numpy(), qdas1.detach().cpu().numpy()\n",
    "\n",
    "    us_1angle = np.stack((idas1, qdas1), axis=0)\n",
    "    us_1angle = torch.from_numpy(us_1angle)\n",
    "\n",
    "    return us_1angle\n",
    "\n",
    "def USData_Nangles(root_dir,file):\n",
    "    print(\"Nangle : \" + str(file))\n",
    "    origin, acq = get_name_and_acq(file)\n",
    "    # print(os.path.abspath(file))\n",
    "    full_path = os.path.abspath(file)\n",
    "    if origin == 'TSH':\n",
    "        P = TSHData(root_dir, acq)\n",
    "        xlims = [P.ele_pos[0, 0], P.ele_pos[-1, 0]]\n",
    "        zlims = [10e-3, 45e-3]\n",
    "\n",
    "    wvln = P.c / P.fc\n",
    "    dx = wvln / 2.5\n",
    "    dz = dx  # Use square pixels\n",
    "    grid = make_pixel_grid(xlims, zlims, dx, dz)\n",
    "    fnum = 1\n",
    "\n",
    "    # make data from 1 angle\n",
    "    x = (P.idata, P.qdata)\n",
    "    dasN = DAS_PW(P, grid, rxfnum=fnum)\n",
    "    idasN, qdasN = dasN(x)\n",
    "    idasN, qdasN = idasN.detach().cpu().numpy(), qdasN.detach().cpu().numpy()\n",
    "    \n",
    "    us_Nangles = np.stack((idasN, qdasN), axis=0)\n",
    "    us_Nangles = torch.from_numpy(us_Nangles)\n",
    "\n",
    "    return us_Nangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the US images from TSH.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.us_images = pd.DataFrame(read_us_images(root_dir))\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.us_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        us_1angle = USData_1angle(self.root_dir,self.us_images.iloc[idx, 0])\n",
    "        us_Nangle = USData_Nangles(self.root_dir,self.us_images.iloc[idx, 0])\n",
    "\n",
    "        sample = {'us_1angle': us_1angle, 'us_Nangle': us_Nangle}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"efficientnet_lite0\"\n",
    "num_outputs = 275544\n",
    "model = build_efficientnet_lite(model_name, num_outputs)\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Optimizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "# US_dataset = USDataset(root_dir = r'D:\\OneDrive\\Documents\\Maestria_Biomedica\\Imagenes_Medicas\\model_cubdl\\TSH')\n",
    "# trainloader = torch.utils.data.DataLoader(US_dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "\n",
    "batch_size = 1\n",
    "US_dataset_train = USDataset(root_dir = r'D:\\OneDrive\\Documents\\Maestria_Biomedica\\Imagenes_Medicas\\model_cubdl\\dataset_split\\train')\n",
    "trainloader = torch.utils.data.DataLoader(US_dataset_train, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "US_dataset_val = USDataset(root_dir = r'D:\\OneDrive\\Documents\\Maestria_Biomedica\\Imagenes_Medicas\\model_cubdl\\dataset_split\\val')\n",
    "valloader = torch.utils.data.DataLoader(US_dataset_val, batch_size=batch_size,shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3w3iaawy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6828... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-flower-5</strong>: <a href=\"https://wandb.ai/marco1337/efficientNet_cubdl/runs/3w3iaawy\" target=\"_blank\">https://wandb.ai/marco1337/efficientNet_cubdl/runs/3w3iaawy</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211214_173130-3w3iaawy\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3w3iaawy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/marco1337/efficientNet_cubdl/runs/1l5rs95n\" target=\"_blank\">chocolate-water-6</a></strong> to <a href=\"https://wandb.ai/marco1337/efficientNet_cubdl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1angle : TSH390.hdf5\n",
      "Nangle : TSH390.hdf5\n",
      "1angle : TSH300.hdf5\n",
      "Nangle : TSH300.hdf5\n",
      "1angle : TSH268.hdf5\n",
      "Nangle : TSH268.hdf5\n",
      "1angle : TSH331.hdf5\n",
      "Nangle : TSH331.hdf5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11200/1480443392.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdasN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cubdl\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cubdl\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cubdl\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             F.sgd(params_with_grad,\n\u001b[0m\u001b[0;32m    137\u001b[0m                   \u001b[0md_p_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                   \u001b[0mmomentum_buffer_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cubdl\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"efficientNet_cubdl\")\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    ckpt = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        das1 = data['us_1angle']\n",
    "        dasN = data['us_Nangle']\n",
    "\n",
    "        das1 = das1.to(device)\n",
    "        dasN = dasN.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # print(das1.shape)\n",
    "        outputs = model(das1)\n",
    "        outputs = torch.reshape(outputs, (2,356,387))\n",
    "        loss = criterion(outputs, dasN)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            wandb.log({\"Train_Loss\": loss})\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "            # PATH = \"model_epoch\" + str(epoch) + \"_ckpt\" + str(ckpt) + \".pt\"\n",
    "            # ckpt = ckpt + 1\n",
    "\n",
    "            # torch.save({\n",
    "            #             'epoch': epoch,\n",
    "            #             'model_state_dict': model.state_dict(),\n",
    "            #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #             }, PATH)\n",
    "    \n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        das1 = data['us_1angle']\n",
    "        dasN = data['us_Nangle']\n",
    "        # forward + backward + optimize\n",
    "        # print(das1.shape)\n",
    "        outputs = model(das1)\n",
    "        outputs = torch.reshape(outputs, (2,356,387))\n",
    "        loss = criterion(outputs, dasN)\n",
    "\n",
    "        # print statistics\n",
    "        wandb.log({\"Val_Loss\": loss})\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, loss))\n",
    "    \n",
    "    \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edc2ed36384d22acd0427a0b15da46384863e4aea8aad8aae706f10aa40d3084"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cubdl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
